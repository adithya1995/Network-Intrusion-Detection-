{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def load_data(data_path, file_name):\n",
    "    csv_path = os.path.join(data_path, file_name)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDD_DATA_PATH = \"data/NSL-KDD\"\n",
    "kdd20 = load_data(KDD_DATA_PATH, \"KDD20Train.csv\")\n",
    "\n",
    "kdd20.columns = [\n",
    "'duration',\n",
    "'protocol_type',\n",
    "'service',\n",
    "'flag',\n",
    "'src_bytes',\n",
    "'dst_bytes',\n",
    "'land',\n",
    "'wrong_fragment',\n",
    "'urgent',\n",
    "'hot',\n",
    "'num_failed_logins',\n",
    "'logged_in',\n",
    "'num_compromised',\n",
    "'root_shell',\n",
    "'su_attempted',\n",
    "'num_root',\n",
    "'num_file_creations',\n",
    "'num_shells',\n",
    "'num_access_files',\n",
    "'num_outbound_cmds',\n",
    "'is_host_login',\n",
    "'is_guest_login',\n",
    "'count',\n",
    "'srv_count',\n",
    "'serror_rate',\n",
    "'srv_serror_rate',\n",
    "'rerror_rate',\n",
    "'srv_rerror_rate',\n",
    "'same_srv_rate',\n",
    "'diff_srv_rate',\n",
    "'srv_diff_host_rate',\n",
    "'dst_host_count',\n",
    "'dst_host_srv_count',\n",
    "'dst_host_same_srv_rate',\n",
    "'dst_host_diff_srv_rate',\n",
    "'dst_host_same_src_port_rate',\n",
    "'dst_host_srv_diff_host_rate',\n",
    "'dst_host_serror_rate',\n",
    "'dst_host_srv_serror_rate',\n",
    "'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate',\n",
    "'attack_type',\n",
    "'unknown number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output: DataFrame\n",
    "kdd20_data = kdd20.drop([\"attack_type\", \"unknown number\"], axis=1)\n",
    "kdd20_labels = kdd20[\"attack_type\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "    \n",
    "kdd20_data_num = kdd20_data.drop([\"protocol_type\",\"service\",\"flag\"], axis=1)\n",
    "    \n",
    "num_attribs = list(kdd20_data_num)\n",
    "cat_attribs = [\"protocol_type\",\"service\",\"flag\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', OneHotEncoder(n_values='auto')),\n",
    "    ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline)\n",
    "    ])\n",
    "\n",
    "kdd20_prepared = full_pipeline.fit_transform(kdd20_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Five Class\n",
    "- nomal - 0\n",
    "- dos  - 1\n",
    "- probe - 2\n",
    "- U2R - 3\n",
    "- R2L - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd20_five_labels = kdd20_labels.replace([\n",
    "'back',\n",
    "'buffer_overflow',\n",
    "'ftp_write',\n",
    "'guess_passwd',\n",
    "'imap',\n",
    "'ipsweep',\n",
    "'land',\n",
    "'loadmodule',\n",
    "'multihop',\n",
    "'neptune',\n",
    "'nmap',\n",
    "'perl',\n",
    "'phf',\n",
    "'pod',\n",
    "'portsweep',\n",
    "'rootkit',\n",
    "'satan',\n",
    "'smurf',\n",
    "'spy',\n",
    "'teardrop',\n",
    "'warezclient',\n",
    "'warezmaster',\n",
    "'normal'], [1,3,4,4,4,2,1,3,4,1,2,3,4,1,2,3,2,1,4,1,4,4,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(kdd20_prepared, kdd20_five_labels, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Linear Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 1.602569\n",
      "iteration 100: loss 0.128250\n",
      "iteration 200: loss 0.114309\n",
      "iteration 300: loss 0.109089\n",
      "iteration 400: loss 0.106420\n",
      "iteration 500: loss 0.104824\n",
      "iteration 600: loss 0.103782\n",
      "iteration 700: loss 0.103058\n",
      "iteration 800: loss 0.102535\n",
      "iteration 900: loss 0.102143\n"
     ]
    }
   ],
   "source": [
    "X = X_train.toarray()\n",
    "y = y_train.values\n",
    "\n",
    "D = 118 # dimensionality\n",
    "K = 5 # number of classes\n",
    "\n",
    "# initialize parameters randomly\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "\n",
    "for i in range(1000):  \n",
    "    # evaluate class scores, [N x K]\n",
    "    scores = np.dot(X, W) + b \n",
    "    \n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "    \n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    loss = data_loss + reg_loss\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print ('iteration %d: loss %f' % (i, loss))\n",
    "        \n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "    \n",
    "    \n",
    "    # backpropate the gradient to the parameters (W,b)\n",
    "    dW = np.dot(X.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "    \n",
    "    dW += reg*W # regularization gradient\n",
    "    \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9734\n"
     ]
    }
   ],
   "source": [
    "# evaluate test set accuracy\n",
    "X = X_test.toarray()\n",
    "y = y_test.values\n",
    "\n",
    "scores = np.dot(X, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print ('test accuracy: %.4f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 1.610169\n",
      "iteration 100: loss 0.093974\n",
      "iteration 200: loss 0.068944\n",
      "iteration 300: loss 0.060447\n",
      "iteration 400: loss 0.057150\n",
      "iteration 500: loss 0.055441\n",
      "iteration 600: loss 0.054439\n",
      "iteration 700: loss 0.053742\n",
      "iteration 800: loss 0.053241\n",
      "iteration 900: loss 0.052863\n"
     ]
    }
   ],
   "source": [
    "X = X_train.toarray()\n",
    "y = y_train.values\n",
    "\n",
    "# initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "\n",
    "for i in range(1000):\n",
    "    # evaluate class scores, [N x K]\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    \n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "    \n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 100 == 0:\n",
    "        print ('iteration %d: loss %f' % (i, loss))\n",
    "        \n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "    \n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    # finally into W,b\n",
    "    dW = np.dot(X.T, dhidden)\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "    \n",
    "    # add regularization gradient contribution\n",
    "    dW2 += reg * W2\n",
    "    dW += reg * W\n",
    "    \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "# evaluate test set accuracy\n",
    "X = X_test.toarray()\n",
    "y = y_test.values\n",
    "\n",
    "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print ('testing accuracy: %.4f' % (np.mean(predicted_class == y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
